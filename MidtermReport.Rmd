---
title: "DATA 698 Midterm Report: Executive Order 88 Energy Analytics & Data Cleansing"
author: "Dan Smilowitz"
date: "November 3, 2017"
output: 
  pdf_document: 
    fig_caption: yes
    fig_height: 5
    fig_width: 7
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = NA, fig.align = "center")
```

## Background
On December 28, 2012, New York Governor Andrew Cuomo issued [Executive Order 88](http://www.governor.ny.gov/news/no-88-directing-state-agencies-and-authorities-improve-energy-efficiency-state-buildings) requiring that

  > By April 1, 2020, all Affected State Entities shall collectively reduce the average EUI [Energy Use Intensity] in State-owned and managed buildings by at least 20% from a baseline of the average EUI of such buildings for State fiscal year 2010/2011

The [New York Power Authority](https://www.nypa.gov) was charged with establishing a central management and implementation team to administer the executive order, directed and authorized to (among other responsibilities):

  - Take all appropriate measures to ensure that the order's target is met
  - Direct Affected State Entities to comply with the requirements of this order
  - Provide strategic, technical, and other assistance to each entity to support implementation
  - Develop annual milestones for achieving the target
  - Develop and implement reporting requirements to document each entity's progress toward the target
  - Develop a comprehensive operations and maintenance plan for the State's building portfolio to help achieve no cost and low cost efficiency improvements and ensure that efficiency savings are sustained

The Executive Order (EO88) led to the creation of NYPA's [BuildSmart NY](https://www.nypa.gov/innovation/programs/buildsmart-ny) program.  BuildSmart NY is a key part of Governor Cuomo's [Reforming the Energy vision](https://rev.ny.gov/) energy strategy and is the premier energy efficiency program referenced in the [2015 New York State Energy Plan](https://energyplan.ny.gov/Plans/2015.aspx).


### Problem Description
In order to track state entities' performance against their 2010/2011 baseline, NYPA collects utility bill information for both electricity and all other fuels (natural gas, propane, water, steam, oil, etc.) for all covered facilities and compares usage to the square footage of each facility.  This information is reported by each agency and submitted to NYPA in the form of an Excel spreadsheet template.

Presently, agencies are responsible for performing their own data validation --- the performance of this task has proved incomplete.  Many facilities have shown an increase in reported EUI relative to the baseline year -- this has, on investigation, been shown to be due to missing reported data for the baseline year.  Additionally, some facilities have been observed to show rather large spikes or dips in their reported data -- these are believed to be due to data entry errors and have been excluded from reporting.

In order to meet its reporting requirements for Executive Order 88, NYPA must ensure accurate data is used for the establishment of baselines, and must continue to ensure that reported data is feasible and not the result of data entry errors.  With the accuracy of the data ensured, the accuracy of performance reports can be enhanced.

Presently, the [annual EO88 report](https://www.nypa.gov/-/media/nypa/documents/document-library/operations/eo88-annualreport-2016.pdf) produced by the BuildSmart NY team identifies performance at the state, agency, and facility levels, but does not sufficiently highlight trends in the data.  The use of advanced analytics will enable the identification of commonalities or differences in performance by facility characteristics to allow for the development of new energy service offerings to help customer meet their required energy reductions.

The data is hosted on a multi-tenant database owned by a software partner that NYPA initially partnered with in 2013.  The vendor has provided extracts of the data, which is organized in a schema that provides a great deal of redundancy and unused database columns; this harms the efficiency of computations based on this data.  Further, the data is currently stored in a "wide" format, with each facility having one row for each fiscal year, containing hundreds of variables (multiple units across many fuels), making analysis more cumbersome.  Finally, the vendor's platform has been unable to properly handle the quality issues of reported data.



### Expected Approaches \& Required Delivery

The method for handling of missing data has not yet been determined, but it is expected that values will be imputed reported data for a given facility using other reported values for that facility, in combination with data for facilities of similar size and type, along with weather data.  The NYEM platform provides weather data through its portal -- this will be used if possible, otherwise another appropriate source of weather data will be identified.  A method for flagging possibly-abberant data will also be established, likely based on percentage change from previous months.  Those data points flagged as potential errors will be compared to values from the same imputation method for missing data -- if there is an intolerable difference between the reported and calculated values, the reported value will be deemed an error and replaced with the imputed value; otherwise the reported data will be deemed feasible and remain.

<!--  expand NYEM explanation -->

In order to allow for filtering of analysis by facility characteristics and fuels, the data will be converted to more condensed format, following the "tidy data" principles proposed by [Hadley Wickham](http://vita.had.co.nz/papers/tidy-data.pdf).  This transformed data will be stored in a properly normalized SQL database.  This is required for two reasons: first, the volume of data makes in-memory storage of the entire dataset impractical; second, the data transformation and migration will be operationalized by NYPA, and it is desired that the data be stored in a Microsoft SQL Server database.

While the analysis for this project will be conducted entirely in R (interfacing with SQL Server), there may be a later migration to leverage the approach of this project into an enterprise ETL (extract, transform, & load) software to allow automated ingestion into a cloud platform.  Similarly, interactive analysis & insights potentially provided through R Shiny application may be migrated to Tableau to better fit into NYPA's enterprise architecture and minimize required infrastructure support.


------------------------------------
\newpage

## Data Processing
The data provided from the software vendor was provided in two files, one containing building data and the other containing reported consumption data.  These files are read into R:

```{r load-data}
library(tidyverse)

bldg <- read_csv("data/2017-10-25_nyem-eo88_bldg-all-sfy.csv")
eo88 <- read_csv("data/2017-10-26_nyem-eo88_consumption-all-sfy.csv")
```

### Building Data
The structure of the building information dataset is investigated:
```{r summ-bldg}
#devtools::install_github("ropenscilabs/skimr")
skimr::skim_with(
  character = list(min = NULL, max = NULL, empty = NULL, n = NULL, complete = NULL),
  integer = list(complete = NULL, n = NULL, mean = NULL, sd = NULL, min = NULL,
                 p25 = NULL, p75 = NULL, max = NULL, hist = NULL),
  numeric = list(complete = NULL, n = NULL, mean = NULL, sd = NULL, min = NULL,
                 p25 = NULL, p75 = NULL, max = NULL, hist = NULL, median = NULL))
skimr::skim(bldg)
```


The `bldg` data frame contains 126 columns, many of which correspond to reporting fields for each fiscal year.  There also a fair number of fields corresponding to building information independent of fiscal year.  To make more efficient use of storage, the data is converted using Hadley Wickham's [tidy data](http://vita.had.co.nz/papers/tidy-data.pdf) principles:
```{r tidy-bldg}
# tidy data that changes by SFY
bldg <- bldg %>%
  gather(Field, Value,
         `SFY2010-11 Gross Floor Area (ft2)`:
           `SFY2016-17 Property Type 5 Percent of Total Gross Floor Area`)

# separate FY from actual measure
library(stringr)
bldg <- bldg %>% 
  mutate(SFY = str_sub(Field, end = 10),
         Field = str_sub(Field, start = 12))

# remove duplicate entries per FY & ESP ID (unique identifier)
bldg <- bldg %>% distinct(`ESP Location ID`, SFY, Field, .keep_all = TRUE)

# spread FY-based fields
bldg <- bldg %>% 
  spread(Field, Value)

# clean up names for easier manipulation
names(bldg) <- str_replace_all(names(bldg), " ", "")
```

Building metadata that remains constant and building data that may change by fiscal year are separated into two tables to join like operations and further optimize use of storage.  The primary key for each building is `ESPLocationID`; as such, this field is contained in both tables:
```{r split-bldg}
# create two dfs -- one with fixed data & hierarchy; other with SFY data
bldg_meta <- bldg %>% select(ESPLocationID:NYSNOAAClimateRegion)
bldg_sfy  <- bldg %>% select(ESPLocationID, SFY:`WeeklyOperatingHours(Hours/Week)`)
```


#### Fixed Building Metadata
The building metadata requires only minor cleanup -- removing duplicate entries (arising from the repeating of these fields across fiscal years) and the renaming of fields
```{r clean-bldg-meta}
# remove duplicates
bldg_meta <- distinct(bldg_meta)

# rename fields
bldg_meta <- bldg_meta %>% 
  rename(Agency = AgencyName,
         ClimateRegion = NYSNOAAClimateRegion,
         Name = BuildingName,
         Address = BuildingAddress,
         City = BuildingCity,
         ZipCode = BuildingZipCode)
```


#### Variable Building Data
Building data varying across fiscal years requires additional cleanup, as the names of fields are lengthy and not all variables are stored as the correct data type due to earlier transformations.

```{r clean-bldg-sfy}
# remove redundant "SFY" from SFY field
bldg_sfy$SFY <- str_sub(bldg_sfy$SFY, 4)

# shorten long names & remove any parentheses
bldg_sfy <- bldg_sfy %>%
  rename(FloorArea = `GrossFloorArea(ft2)`,
         PeakOccupants = PeakTotalOccupants,
         CooledPct = PercentofGrossFloorAreaCooled,
         HeatedPct = PercentofGrossFloorAreaHeated,
         Type_BEDES = `PropertyType(BEDES)`,
         Type_ES = `PropertyType(ES)`,
         WeeklyHours = `WeeklyOperatingHours(Hours/Week)`)

# shorten property & percentage
names(bldg_sfy) <- str_replace_all(names(bldg_sfy), "Property", "")
names(bldg_sfy) <- str_replace_all(names(bldg_sfy), "PercentofTotalGrossFloorArea", "Pct")

# drop empty "Type" field
bldg_sfy <- bldg_sfy %>% select(-Type)

# convert fy-reported fields from character to numeric
bldg_sfy <- mutate_at(bldg_sfy,
  vars(FloorArea, PeakOccupants, CooledPct, HeatedPct, Type1Pct, Type2Pct,
       Type3Pct, Type4Pct, Type5Pct, WeeklyHours),
  parse_number)
```

The two tables are now far more reasonable and are properly normalized:
```{r summ-newbldg}
skimr::skim(bldg_meta)
skimr::skim(bldg_sfy)
```

